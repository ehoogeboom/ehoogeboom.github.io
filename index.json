[{"authors":["admin"],"categories":null,"content":"My name is Emiel Hoogeboom and I am a PhD student at the University of Amsterdam under the supervision of Max Welling, in the UvA-Bosch Delta Lab.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ehoogeboom.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"My name is Emiel Hoogeboom and I am a PhD student at the University of Amsterdam under the supervision of Max Welling, in the UvA-Bosch Delta Lab.","tags":null,"title":"Emiel Hoogeboom","type":"authors"},{"authors":["Emiel Hoogeboom"],"categories":[],"content":" A quick motivation. A nice application of our E(n) Normalizing Flow (E-NF) is the simultaneous generation of molecule features and 3D positions. However the method also aimed to be general-purpose and can be used for other data as well. You can think about point-cloud data, or even better point-cloud data with some features on the point (like a temperature). E-NFs can learn a distribution over data like that. After learning such a distribution, we can sample new points that resemble the data, if successful ;).\nThe aim of this blog post is to guide you through some of the techniques we used to make E(n) Equivariant Flows (paper). I really liked working on this project because it nicely brought together several topics in literature. For some of these, there are related previous projects that I was a part of. To give an overview, this blog is on:\n Normalizing Flows Continuous Time Normalizing Flows E(n) Equivariant GNNs Argmax Flows And then finally tying everything together for E(n) Normalizing Flows.  These sections are aimed to be as much stand-alone as possible, although they are of course connected. Also, this write-up is intended to be an intro, and is should not be seen as comprehensive or complete. For more details and a more thorough exposition, please see the paper. I hope you will enjoy it :)\n1. Normalizing Flows and the change of variables formula Main takeaway Normalizing flows are invertible functions. We generally name the flow $f$ and its inverse $g = f^{-1}$, which you can also name \u0026ldquo;flow\u0026rdquo;. The flow contains neural networks and can be learned from data. Flows allow exact likelihood computation via the change of variables formula. The difficult part is often finding a nice (learnable) function that is invertible. After training, flows can be used to generate data. Confession: To this date I am still unsure why they are called \u0026ldquo;normalizing\u0026rdquo;. Some people say because they map to a normal distribution, others say because they re-normalize the distribution after the map.\nA Generative Model. A generative model is often defined using a simple distribution $p_Z$ and a learnable function $g: \\mathbf{z} \\mapsto \\mathbf{x}$. For the simple distribution we desire something that we can sample from, and we call this the base distribution. This distribution lives in some other space than the data itself, called the latent space. We will name a variable of this space $\\mathbf{z}$ and without much imagination we will pick the base distribution over $\\mathbf{z}$ to be Gaussian, so $p_Z(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} | 0, 1)$.\nNext we want to initialize some neural network to model $g: \\mathbf{z} \\mapsto \\mathbf{x}$. However, without any constraints on $g$, in practice we can only sample from this model. Sampling works like this: First sample $\\mathbf{z} \\sim p_Z$ (using torch.randn() for instance) and then compute $\\mathbf{x} = g(\\mathbf{z})$. This procedure implies a distribution $p_X(\\mathbf{x})$. Okay so sampling is easy. The difficulty is optimizing such a model. Given a new datapoint $\\mathbf{x}$ it is extremely difficult to compute the likelihood $p_X(\\mathbf{x})$.\nLet\u0026rsquo;s see an example in 1D:  A simple distribution $p_Z$ is fed through a function $\\mathbf{x} = g(\\mathbf{z})$ to give $p_X$  \nMagic! By composing a simple distribution with a somewhat complicated function $g$, we have created a distribution with two peaks from a unimodal Gaussian. To build some intuition, think about the following: If the derivative of $g$ is high, will the resulting density in $p_X$ at that point become lower or higher? To reiterate the point of the previous paragraph: it is generally difficult to compute $p_X$. Huh? But how did we do it in this example then? Secretly, the example has been handcrafted so that $g$ is invertible. It turns out, $g$ is already a flow\u0026hellip;\nNormalizing Flows. To compute $p_X(\\mathbf{x})$, Normalizing Flows (Rezende \u0026amp; Mohamed 2015, Dinh et al. 2016) come to the rescue, they restrict $g$ to be invertible. As a result we can utilize the change of variables formula. In 1 dimension it reads: $$p_X(\\mathbf{x}) = p_Z(g^{-1}(\\mathbf{x})) \\left| g\u0026rsquo;(g^{-1}(\\mathbf{x})) \\right|^{-1}.$$ with $g\u0026rsquo;$ as derivative. In more dimensions the formula is: $$p_X(\\mathbf{x}) = p_Z(g^{-1}(\\mathbf{x})) \\left | \\det J_g(g^{-1}(\\mathbf{x})) \\right|^{-1}.$$ That is interesting, we find the likelihood of a point $\\mathbf{x}$ by inverting $g$ and going back to the corresponding latent point $\\mathbf{z}$. Also we are using the Jacobian determinant of the function at some point. Funny is that there is a lot of inverses $g^{-1}$ in the formula. It turns out we need the inverse more than $g$ itself! For that reason we often learn the inverse directly and call this $f = g^{-1}$. This is possible because if $f$ is invertible, then $g$ is too. It\u0026rsquo;s only changing the perspective a little. After writing everything in terms of $f$ the change-of-variables formula looks like this:\n$$p_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\left | \\det J_f(\\mathbf{x}) \\right|, \\quad \\mathbf{z} = f(\\mathbf{x}).$$\nOkay, what does this formula mean? First of all it\u0026rsquo;s good to recall that we are interested in learning a complex model distribution, something that can capture all the intricate dependencies of our data. We call this model distribution $p_X$. Ideally, we want a sample $\\mathbf{x} \\sim p_X$ to look like a sample from the data, after we\u0026rsquo;ve trained our model. So this formula tells us that given a datapoint $\\mathbf{x}$, we should transform it using $\\mathbf{z} = f(\\mathbf{x})$ and compute two things: $p_Z(\\mathbf{z})$ and the Jacobian determinant $\\det J_f(\\mathbf{x})$. Multiply these things and we get the likelihood $p_X(\\mathbf{x})$.\nGoing back to the example, we can look at $f$, which is the inverse of $g$:  The inverse of $g$  \nUsing this function $f$ and given a data distribution, we can now do some inference, that is we can compute the likelihood of datapoints under our model, and observe how it compares to the true data distribution:\n Data and model distribution. $f$ tries to shape data into a Gaussian.   Awesome! We now have a method to learn a model distribution $p_X$ that fits to samples from some dataset. In practice training looks like this: We get a datapoint $\\mathbf{x}$. We compute the corresponding latent point $\\mathbf{z} = f(\\mathbf{x})$ and then compute $p_Z(\\mathbf{z})$ and $|\\det J_f(\\mathbf{x})|$. We then multiply these together to get the model likelihood of the datapoint $\\mathbf{x}$. The function $f$ is optimized to maximize $p_X(\\mathbf{x})$. Sampling goes like this: Sample $\\mathbf{z} \\sim p_Z$ (using torch.randn()) and then compute $\\mathbf{x} = g(\\mathbf{z}) = f^{-1}(\\mathbf{z})$. A small detail: In higher dimension log-space generally works a lot better for optimization and in that case the change-of-variable looks like: $$\\log p_X(x) = \\log p_Z(z) + \\log \\left | \\det J_f(x) \\right|, \\quad z = f(x).$$\nThis is the intro to normalizing flows. There is already a much better blogpost written by Jakub here, but I made you read through my version first :). Be sure to check it out for more details!\n2. Continuous-time Normalizing Flows Main takeaway Continuous-time Normalizing Flows come from a beautiful insight: solutions to ordinary differential equations (ODEs) are almost always invertible (under some very mild constraints). This is awesome, the reason: ODEs are a very easy way to build a flow, which we call continuous-time flows. And we only need to put mild constraints on the neural network $\\phi$ inside the ODE, mainly some differentiability. For an illustration of a 1 dimensional continuous-time flow see below:\n A continuous-time flow generates a more complicated 1D distribution from a Gaussian. This visualization was inspired by the figure from Grathwohl et al. (2018)   The Transformation An ODE can be formulated like this: $$\\mathbf{z} = \\mathbf{x} + \\int_0^1 \\phi(\\mathbf{x}(t)) dt$$ where $\\phi$ models the first order derivative with respect to time. Only time is not really time, it\u0026rsquo;s more a conceptual thing to think about the ODE. We will not go into detail into how to solve such a thing, but rest assured that there is a beautiful python package written by Chen et al. (2018) to do so called torchdiffeq. In code solving the above really amounts to calling z = odeint(self.phi, x, [0, 1]). To me, this simplicity is magical. To connect this to the previous section, we say that $f$ gives the solution to the ODE, so that $\\mathbf{z} = f(\\mathbf{x})$.\nAs claimed before, the inverse to this equation exists. It is: $$\\mathbf{x} = \\mathbf{z} + \\int_1^0 \\phi(\\mathbf{x}(t)) dt$$ and is solved by calling x = odeint(self.phi, z, [1, 0]). Again, we take this entire transform and call it $g= f^{-1}$ from the previous section. Since it\u0026rsquo;s invertible, we can use it as a flow: a continuous-time normalizing flow (Chen et al 2017, Chen et al. 2018)\nContinuous-time change-of-variables. Just having an invertible function is not enough. We also need to find $J_f(\\mathbf{x})$ to compute the log-likelihood of a datapoint $\\mathbf{x}$. We can find that the continuous time change-of-variables is written as:\n$$\\log p_X(\\mathbf{x}) = \\log p_Z(\\mathbf{z}) + \\int_0^1 \\mathrm{Tr }\\, J _{\\phi}(\\mathbf{x}(t)) dt$$\nIt turns out that $\\log \\det J_f = \\int_0^1 \\mathrm{Tr }\\, J _{\\phi}(\\mathbf{x}(t)) dt$. So the change of variables formula is written as another ODE. By the way, this can also be solved using odeint. Without giving a formal proof, there is a nice identity via the matrix logarithm that you can use to see the connection between log determinant and the trace. The identity is: $\\log \\det J = \\mathrm{Tr }\\, \\log J$. Observe that this second $\\log$ is the matrix logarithm. Imagine taking a small step (like in Euler\u0026rsquo;s method) for small $\\varepsilon$ as stepsize: $\\mathbf{x}\u0026rsquo; = \\mathbf{x} + \\varepsilon \\cdot \\phi(\\mathbf{x})$. Then the log Jacobian determinant of that tiny step can be written as:\n$$\\log \\det (I + \\varepsilon J_ \\phi) = \\mathrm{Tr }\\, \\log (I + \\varepsilon J_ \\phi) = \\mathrm{Tr }\\, \\sum_{i=1}^\\infty (-1)^{i+1} \\frac{(\\varepsilon {J_\\phi})^i}{i} =\\mathrm{Tr }\\, (\\varepsilon {J_\\phi}) + O(\\varepsilon^2)$$\nSo in summary $\\log \\det (I + \\varepsilon J_ \\phi) = \\varepsilon \\mathrm{Tr }\\, ({J_\\phi})$ as $\\varepsilon \\to 0$. Speaking informally, adding up all these small $\\varepsilon$ terms and taking the limit when $\\varepsilon \\to 0$ then gives $\\int_0^1 \\mathrm{Tr }\\, J _{\\phi}(\\mathbf{x}(t)) dt$.\nVisualization of the dynamics \u0026amp; generation Let\u0026rsquo;s look at a 2D Dataset: the eight Gaussians. From now on we\u0026rsquo;ll be looking at actual (data) points instead of density curves as in the 1D examples before. First let\u0026rsquo;s examine the data, here are some samples from the dataset called 8 Gaussians:  A 1000 datapoints from the 8 Gaussians dataset.  \nNow we already trained a neural network $\\phi$ to do well on this task. You can see in the below image how it transforms the datapoints into the Gaussian base distribution over time:  Given data $\\mathbf{x}$ transform to $\\mathbf{z} = \\mathbf{x} + \\int_0^1 \\phi(\\mathbf{x}(t)) dt$. The arrows show the output of $\\phi$  \nThe arrow directions in the plots really show the output of $\\phi$ at those points. $\\phi$ is literally describing how to points should move through the space. Interestingly, for generation we go in the opposite direction, so $-\\phi$. To generate points, first we sample Gaussian noise in $\\mathbf{z} \\sim \\mathcal{N}(0, 1)$ and then compute the reverse of the flow:  Given noise $\\mathbf{z}$ generate $\\mathbf{x} = \\mathbf{z} + \\int_1^0 \\phi(\\mathbf{x}(t)) dt$. The arrows show the output of $-\\phi$  \nThat\u0026rsquo;s it for continuous-time flows. They are ODEs and have their own continuous time change-of-variables formula that itself is also an ODE.\nSome remarks Several details have not been explained. How to compute the gradient efficiently? (Adjoint method). How to compute the trace efficiently? (Hutchinson\u0026rsquo;s trace estimator). For these details please see the paper by Chen et al. (2018) and Grathwohl et al. (2018). Also $\\phi$ may depend on time, but in both notation and visualization it was easier to ignore this.\n3. E(n) Equivariant GNNs Main takeaway Equivariance is about the very intuitive concept of symmetry: If my input rotates, my output needs to rotate similarly. In our work, we are interested in rotations, reflections and translations, which is linked to the Euclidean Group E(n). Therefore, we want a network $\\phi$ that is E(n) equivariant. For this we use EGNNs, which are computationally cheap and expressive.\nThen the flow $f$ constructed from $\\phi$ is also equivariant. And then with this equivariant flow $f$ an invariant distribution $p_X$ can be constructed. This distribution has the desirably property that no matter how you rotate your input, it will give the same likelihood.\n E-GNNs, the network $\\phi$ is equivariant to rotations and translations.   Equivariance Background Group Equivariant Networks (Cohen \u0026amp; Welling 2016, Dieleman et al. 2016) hard-code symmetries in their transformations. Taking rotations with a matrix $\\mathbf{Q}$ as an example: Rotating the input should also rotate the output of a function. Or concisely, $\\phi$ is rotation equivariant if: $$\\phi(\\mathbf{Q}\\mathbf{x}) = \\mathbf{Q}\\phi(\\mathbf{x}) \\quad \\text{ for all rotation matrices } \\quad \\mathbf{Q}$$\nAlthough the statement might seem technical at first, it\u0026rsquo;s conveying a very natural constraint that we can all relate to: for some structures it does not matter whether you rotate them or not. And the predictions should be the same, or rotate accordingly.\nThe Euclidean Group E(n), and how does it act? The group of translations, rotations and reflections is called the Euclidean group, and is referred to as $E(n)$ for short. An element of this group can be described by an $n \\times n$ orthogonal matrix $\\mathbf{Q}$ and translation vector $\\mathbf{t} \\in \\mathbb{R}^n$. The group is not the full story though, one has to decide how the group acts on the data. Here you want to make choices that fit nature, and although it seems technical it is something that everybody intuitively already understands. Imagine a graph with points that have coordinates $\\mathbf{x}$ and a temperature $\\mathbf{h}$. Then a rotation and translation act on $\\mathbf{x}$ by rotating and translating it, but the same act on the temperature $\\mathbf{h}$ will not change it. For this reason we sometimes refer to features $\\mathbf{h}$ as invariant. There is a third type of data: velocities $\\mathbf{v}$. Although these will rotate like positions, they will not translate. To extend our equivariant notation from earlier, we say $\\phi$ is E(n) equivariant if:\n$$\\phi(\\mathbf{Q}\\mathbf{x}, \\mathbf{h}) = \\mathbf{Q}\\mathbf{z}_x, \\mathbf{z}_h \\quad \\text{where} \\quad \\mathbf{z}_x, \\mathbf{z}_h = \\phi(\\mathbf{x}, \\mathbf{h})$$\nThere is a large body of work dedicated to finding expressive networks equivariant to Euclidean symmetries, for example Tensor Field Networks (Thomas et al. 2018).\nE-GNNs In E-GNNs (Satorras et al. 2021) we aim to simplify E(n) Equivariant transformations as much as possible, while retaining or improving the performance. The main take-away is the following: Most of the complexity of the transformation is learned via the edge function $\\phi_e$ the position function $\\phi_x$ and $\\phi_h$. Because all these functions operate on an invariant representation, they can be any arbitrary function. To see the invariance: Just imagine rotating the $\\mathbf{x}$\u0026rsquo;s, these functions will not change, because distances $|\\mathbf{x}_{i}^{l}-\\mathbf{x}_{j}^{l}|^{2}$ do not change under rotations. $$\\mathbf{m}_{ij} =\\phi_{e}\\left(\\mathbf{h}_{i}^{l}, \\mathbf{h}_{j}^{l},\\left|\\mathbf{x}_{i}^{l}-\\mathbf{x}_{j}^{l}\\right|^{2}\\right) \\quad \\text{ and }\\quad \\mathbf{m}_{i} = \\sum_{j \\not= i} e_{ij}\\mathbf{m}_{ij}, $$\nfor the messages. The positions and invariant features are updated using:\n$$\\mathbf{x}_{i}^{l+1} =\\mathbf{x}_{i}^{l}+\\sum_{j \\neq i} \\frac{(\\mathbf{x}_{i}^{l}-\\mathbf{x}_{j}^{l})}{|\\mathbf{x}_{i}^{l}-\\mathbf{x}_{j}^{l}| + 1} \\phi_{x}\\left(\\mathbf{m}_{ij}\\right) \\quad \\text{ and }\\quad \\mathbf{h}_{i}^{l+1} =\\phi_{h}\\left(\\mathbf{h}_{i}^l, \\mathbf{m}_{i} \\right)$$\nTogether these layers define a single layer. We can just stack them to get a deeper E-GNN. Note that in the update equation for $\\mathbf{x}$ we show the more stable version which we introduced in E(n) Flows, which was important to improve the stability of the ODE.\nInvariant Distributions with Equivariant Flows In the end we want the likelihood of a point cloud to remain the same under rotations. So we desire: $$p_X(\\mathbf{x}) = p_X(\\mathbf{Q}\\mathbf{x}) \\quad \\text{ for all orthogonal matrices } \\quad \\mathbf{Q}$$\nKöhler et al. (2019) showed a really cool result: Take an equivariant flow $f$ with an invariant base distribution $p_Z$ with $p_Z(\\mathbf{z}) = p_Z(\\mathbf{Qz})$. Then together they give a complicated but invariant likelihood $p_X$. In an equation this can be seen from:\n$$p_X(\\mathbf{Q}\\mathbf{x}) = p_Z(f(\\mathbf{Q}\\mathbf{x})) |\\det J_f(\\mathbf{Q}\\mathbf{x}))| = p_Z(\\mathbf{Q}f(\\mathbf{x})) |\\det \\mathbf{Q} J_f(\\mathbf{x}))| = p_Z(f(\\mathbf{x})) |\\det J_f(\\mathbf{x}))| $$\nYou might ask why not take an invariant flow? So something with $f(\\mathbf{Qx}) = f(\\mathbf{x})$? Then you would also have an invariant distribution $p_X$. Good thinking! However, it turns out that in that case the transformations that you can learn are very limited, and as a result you cannot really model complicated invariant distributions with $p_X$. A final remark: In this section we only talked about rotations, for translations the story is slightly different and for details please see the paper.\n4. Argmax Flows Main takeaway We have a problem, we have a continuous-time flow, which models continuous distributions. However, some features are discrete, like for instance an atom type (Carbon, Hydrogen, Oxygen, \u0026hellip;). To lift these discrete values, we use Argmax Flows. Argmax Flows give a way to transition between the categorical and continuous. To discretize we can simply take the argmax: $\\mathbf{h} = \\mathrm{argmax}\\,\\,\\boldsymbol{\\tilde{h}}$. To lift to the continuous, we sample $\\boldsymbol{\\tilde{h}} \\sim q(\\cdot | \\mathbf{h})$. To optimize, we only need to subtract $\\log q(\\boldsymbol{\\tilde{h}} | \\mathbf{h})$ from the objective, and then we are guaranteed to learn a lowerbound on our discrete log-likelihood.\n Argmax Flows   First of all, in case of ordinal data (integers like a temperature), Variational Dequantization by by Ho et al. (2019) is exactly what you want. In this section we will focus on categorical data. In the case, we can use Argmax Flows (Hoogeboom et al. 2021). These work as follows: Assume a continuously distributed variable $p(\\boldsymbol{\\tilde{h}})$ and let $\\mathbf{h} = \\mathrm{argmax}\\,\\,\\boldsymbol{\\tilde{h}}$. For $K$ classes we have $\\boldsymbol{\\tilde{h}} \\in \\mathbb{R}^K$ and $\\mathbf{h} \\in \\{0, 1, \\ldots, K\\}$. This construction gives us a generative model that outputs classes in $\\{0, 1, \\ldots, K\\}$\nNow observe the following: we can see the deterministic argmax as a discrete distribution with 100% of the mass on that output. So we say $P(\\mathbf{h} | \\boldsymbol{\\tilde{h}}) = 1\\,$ if $\\,\\mathbf{h} = \\mathrm{argmax} \\,\\,\\boldsymbol{\\tilde{h}}\\,$ and $\\,P(\\mathbf{h} | \\boldsymbol{\\tilde{h}}) = 0$ otherwise. This is cool, because sampling from this distribution is exactly the same as taking the argmax.\nWe can now write the entire thing as a latent variable model and derive for a discrete distribution we define: $p(\\mathbf{h}) = \\int P(\\mathbf{h} | \\boldsymbol{\\tilde{h}}) p(\\boldsymbol{\\tilde{h}}) \\mathrm{d}\\boldsymbol{\\tilde{h}}$. Although this integral may seem complicated, it\u0026rsquo;s just counting for a class $\\mathbf{h}$ how much probability mass is in the corresponding continuous region. We then derive in log-space using variational inference: $$ \\log p(\\mathbf{h}) = \\log \\int P(\\mathbf{h} | \\boldsymbol{\\tilde{h}}) p(\\boldsymbol{\\tilde{h}}) \\mathrm{d}\\boldsymbol{\\tilde{h}} \\quad \\text{from definition}$$\n$$ =\\log \\int \\frac{q(\\boldsymbol{\\tilde{h}} | \\mathbf{h})}{q(\\boldsymbol{\\tilde{h}} | \\mathbf{h})} P(\\mathbf{h} | \\boldsymbol{\\tilde{h}}) p(\\boldsymbol{\\tilde{h}}) \\mathrm{d}\\boldsymbol{\\tilde{h}} \\quad \\text{multiply by 1}$$\n$$ =\\log \\mathbb{E}_{\\boldsymbol{\\tilde{h}} \\sim q(\\cdot | \\mathbf{h})} \\frac{P(\\mathbf{h} | \\boldsymbol{\\tilde{h}}) p(\\boldsymbol{\\tilde{h}})}{q(\\boldsymbol{\\tilde{h}} | \\mathbf{h})} \\quad \\text{integral to expectation}$$\n$$ \\geq \\mathbb{E}_{\\boldsymbol{\\tilde{h}} \\sim q(\\cdot | \\mathbf{h})} \\log \\frac{P(\\mathbf{h} | \\boldsymbol{\\tilde{h}}) p(\\boldsymbol{\\tilde{h}})}{q(\\boldsymbol{\\tilde{h}} | \\mathbf{h})} \\quad \\text{Jensen\u0026rsquo;s inequality}$$\nFor this last step, restrict $q(\\boldsymbol{\\tilde{h}} | \\mathbf{h})$ to only have support over the relevant region: the region where $P(\\mathbf{h} | \\boldsymbol{\\tilde{h}}) = 1$. Then we get\n$$ \\log p(\\mathbf{h}) \\geq \\mathbb{E}_{\\boldsymbol{\\tilde{h}} \\sim q(\\cdot | \\mathbf{h})} [\\log p(\\boldsymbol{\\tilde{h}}) - \\log q(\\boldsymbol{\\tilde{h}} | \\mathbf{h})] \\quad \\text{restrict $q$, expand log}$$ And this is exactly the objective that we optimize. What\u0026rsquo;s cool is that this gives us a very nice method to transition between the categorical space $\\mathbf{h}$ and the continuous space $\\boldsymbol{\\tilde{h}}$ using an argmax function in one direction, and sampling from $q$ in the other direction. The only thing we have to take into account is the additional term $- \\log q(\\boldsymbol{\\tilde{h}})$. Now we are completely free to train any continuous distribution (such as a continuous-time flow) for $p(\\boldsymbol{\\tilde{h}})$. This is great because $\\mathbf{x}$ was already continuous, so to optimize a flow on both positions and features $(\\mathbf{x}, \\mathbf{h})$ jointly we needed to lift the features $\\mathbf{h}$ to the continuous $\\boldsymbol{\\tilde{h}}$. We then train on $(\\mathbf{x}, \\boldsymbol{\\tilde{h}})$.\nBecause we can now transition between discrete and continuous so easily, in remaining sections we may not properly distinguish between $\\mathbf{h}$ and $\\boldsymbol{\\tilde{h}}$ anymore. Also sometimes we drop the tilde as in the paper so $\\boldsymbol{\\tilde{h}} = \\boldsymbol{h}$.\nHow to construct q? This section is not super important, but just for the interested reader. We will give the simplest method we came up with. Recall that we need to sample values $\\boldsymbol{\\tilde{h}}$ but only in the region where $\\mathbf{h} = \\mathrm{argmax} \\,\\,\\boldsymbol{\\tilde{h}}$, where $\\mathbf{h}$ is given as a datapoint. First construct a distribution with free noise $\\boldsymbol{u}$, in our case we\u0026rsquo;ll use a Gaussian $\\boldsymbol{u} \\sim \\mathcal{N}(\\mu(\\mathbf{h}), \\sigma(\\mathbf{h}))$, where $\\mu, \\sigma$ are functions modelled by a (shared) EGNN. Then call $i = \\mathbf{h}$ to clarify that it\u0026rsquo;s an index. We leave the $i$\u0026lsquo;th index the same so that $\\boldsymbol{\\tilde{h}}_i = \\boldsymbol{u}_i$. Call this value $T = \\boldsymbol{\\tilde{h}}_i$. Then all the other indices are thresholded using $\\boldsymbol{\\tilde{h}}_{-i} = T - \\mathrm{softplus}(T - \\boldsymbol{u}_{-i}))$, which ensures they are smaller than $T$. So this is exactly the desired argmax constraint. Since the function is bijective, we can use the change-of-variables formula again to find:\n$$\\log q(\\boldsymbol{\\tilde{h}} | \\mathbf{h}) = \\log \\mathcal{N}(\\boldsymbol{u} | \\mu(\\mathbf{h}), \\sigma(\\mathbf{h})) - \\log |\\det \\mathrm{d}\\boldsymbol{\\tilde{h}}/\\mathrm{d}\\boldsymbol{u}|,$$\nusing the derivatives of the softplus. And this is exactly the additional term $\\log q(\\boldsymbol{\\tilde{h}} | \\mathbf{h})$ that we need in the objective so we are done :).\n5. E(n) Equivariant Flows Wow, you made it. And there is good news: we\u0026rsquo;ve been setting up all the relevant parts above so that it fits perfectly for E(n) Equivariant Flows. So most of the hard work is done. First as a reward, let\u0026rsquo;s look at the generation of a molecule using the E-NF:  Animation of E(n) Flows.  \nThe animation is showcasing two properties of our model. 1) The flow is invertible. This is shown via the generate and inference animations. 2) The flow is equivariant. If not, after rotating we wouldn\u0026rsquo;t necessarily arrive at the same molecule (but rotated). Only because the model is both invertible and equivariant, we are able to loop this gif.\nThe Normalizing Flow So what do we need? A description for the positions of the nodes: $\\mathbf{x} \\in \\mathbb{R}^{M \\times n}$. So there $M$ nodes in a $n$-dimensional space. Also we need something for features on the nodes, like an atom type, which we call $\\mathbf{h} \\in \\mathbb{R}^{M \\times \\mathrm{nf}}$, which can contain $\\mathrm{nf}$ features. We know now that if $\\mathbf{h}$ is categorical, then we can lift them to a continuous version using Argmax Flows. Here $f$ maps $\\mathbf{x}, \\mathbf{h} \\mapsto \\mathbf{z}_{x}, \\mathbf{z}_{h}$. Then, we join everything together into a change of variables formula:\n$$p(\\mathbf{x}, \\mathbf{h}) = p_Z(\\mathbf{z}_{x}, \\mathbf{z}_{h}) | \\det J_f |,$$\nwhere $J_f = \\frac{\\mathrm{d}(\\mathbf{x}, \\mathbf{h})}{\\mathrm{d}(\\mathbf{z}_{x}, \\mathbf{z}_{h})}$ is the Jacobian, where all tensors are vectorized for the Jacobian computation. To build $f$, we utilize an ODE with an E-GNN $\\phi$ that is the actual learnable part of our model:\n$$\\mathbf{z}_x, \\mathbf{z}_h = f(\\mathbf{x}, \\mathbf{h}) = [\\mathbf{x}, \\mathbf{h}] + \\int_{0}^{1} \\phi(\\mathbf{x}(t), \\mathbf{h}(t))\\mathrm{d}t.$$\nAs mentioned before, the solution to this integral is simply solved by calling z = odeint(self.phi, x, [0, 1]). In the next section we can describe the dynamics $\\phi$.\nImagine how cool these dynamics are by the way: Previously in the 2D example $\\phi$ was only predicting the velocity for 2D points independently. Now it\u0026rsquo;s way cooler: It\u0026rsquo;s predicting for a collection of points (for instance a molecule with atoms) how each atom should move depending on all the others. If you look at the visualization, $\\phi$ is really deciding how each atom travels through the space.\nThe Dynamics Using the construction of the EGNN layer in the previous section, we can just stack them to get an EGNN. In the experiments we used 6 layers, so $L=6$. The invariant output $\\mathbf{h}^L$ can be used immediately, we denote this $\\mathbf{h}^L(t)$ with $(t)$ just because the input depends on a specific time $t \\in [0, 1]$. A problem would arise if you used $\\mathbf{x}^L(t)$ immediately: the ODE would not be equivariant anymore. Instead, we need the dynamics for $\\mathbf{x}$ to behave like a velocity vector: It should rotate, but not translate under the group actions. The reason for this is that $\\mathbf{x}$ itself already translates under group actions, so just adding the output $\\mathbf{x}^L(t)$ would double that action, essentially translating the output twice. This is easily alleviated by using the residual $\\mathbf{x}^L(t) - \\mathbf{x}(t)$, which does behave as a velocity. We can then write the dynamics of our model, the main learnable component as follows:\n$$\\phi(\\mathbf{x}(t), \\mathbf{h}(t)) = \\mathbf{x}^L(t) - \\mathbf{x}(t), \\mathbf{h}^L(t) \\quad \\text{ where } \\quad \\mathbf{x}^L(t), \\mathbf{h}^L(t) =\\text{EGNN}[\\mathbf{x}(t), \\mathbf{h}(t)].$$\nAnd that\u0026rsquo;s it. We now have a model that can be trained on molecule-like data.\nSamples Training takes about a week or two. Continuous-time normalizing flows are just really expensive, requiring hundreds of evaluations to solve the ODE. Nevertheless, the results are really awesome. We show our method far outperforms existing normalizing flow methods on several datasets. Among these is qm9 which contains molecules, and from that model we can show these samples we generated:\n Samples from the E(n) Flow trained on qm9.   An overview And then the overview of what we actually do during training. The flowchart below shows the steps:  Training E(n) Equivariant Flows   These can be summarized as: 1) Get some datapoint $\\mathbf{x}, \\mathbf{h}$. 2) Lift the discrete features from $\\mathbf{h}$ to the continuous $\\boldsymbol{h}$. 3) Run odeint using the dynamics $\\phi$ modelled by an EGNN. 3) Collect all the terms for the likelihood among which is the base distribution on the output: $\\log p_Z(\\mathbf{z}_x, \\mathbf{z}_h)$. To deal with different molecule sizes, an addition 1D categorical distribution $p(M)$ (think of a histogram) models the molecule size. At last, we can show the objective in its entirety, which brings everything together in a single line: $$\\log p(\\mathbf{x}, \\mathbf{h}) \\! \\geq \\! \\mathbb{E}_{\\boldsymbol{\\tilde{h}} \\sim q(\\cdot | \\mathbf{x}, \\mathbf{h})} [\\underbrace{\\log p_Z(\\mathbf{z}_x, \\mathbf{z}_h)}_{\\text{base likelihood}} + \\underbrace{\\int_0^1 \\mathrm{Tr }\\, J_{\\phi}(\\mathbf{x}(t), \\boldsymbol{\\tilde{h}}(t))dt}_{\\text{ODE volume change}} \\, \\underbrace{-\\log q(\\boldsymbol{\\tilde{h}} | \\mathbf{x}, \\mathbf{h})}_{\\text{lifting term}} + \\underbrace{\\log p(M)}_{\\text{size likelihood}}],$$ where $\\mathbf{z}_x, \\mathbf{z}_h = [\\mathbf{x}, \\mathbf{h}] + \\int_{0}^{1} \\phi(\\mathbf{x}(t), \\mathbf{h}(t))\\mathrm{d}t$.\nConclusions And that\u0026rsquo;s it. In spite of the good results there are some limitations: 1) Continuous-time flows are computationally expensive. 2) The combination of the ODE with the original EGNN is sometimes unstable. After our modification, we still noticed some rare peaks in the loss of the QM9 experiment that can diverge. (Of course when the model is saved you can simply restart from right before that point in training). 3) The model in its current form does not model distributions over edges between nodes. 4) Our likelihood estimation is invariant to reflections, but some structures (like molecules) may be chiral: their mirror image does not interact in the same way as the original. Also, if you are specifically interested in molecule generation also have a look at the work by Gebauer et al. (2019). And that\u0026rsquo;s it, thank you for reading :).\nReferences (in order of appearance) Victor Garcia Satorras, Emiel Hoogeboom, Fabian B. Fuchs, Ingmar Posner, Max Welling. E(n) Equivariant Normalizing Flows. (2021)\nDanilo Jimenez Rezende, Shakir Mohamed. Variational Inference with Normalizing Flows. (2015).\nLaurent Dinh, Jascha Sohl-Dickstein, Samy Bengio. Density estimation using Real NVP. (2016)\nChangyou Chen, Chunyuan Li, Liqun Chen, Wenlin Wang, Yunchen Pu, Lawrence Carin. Continuous-Time Flows for Efficient Inference and Density Estimation (2017)\nRicky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud. Neural Ordinary Differential Equations. (2018)\nWill Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, David Duvenaud. FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models (2018)\nTaco Cohen, Max Welling. Group Equivariant Convolutional Networks. (2016)\nSander Dieleman, Jeffrey De Fauw, Koray Kavukcuoglu. Exploiting Cyclic Symmetry in Convolutional Neural Networks. (2016)\nNathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, Patrick Riley. Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds. (2018)\nVictor Garcia Satorras, Emiel Hoogeboom, Max Welling. E(n) Equivariant Graph Neural Networks. (2021)\nJonas Köhler, Leon Klein, Frank Noé. Equivariant flows: sampling configurations for multi-body systems with symmetric energies. (2019)\nJonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, Pieter Abbeel. Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design. (2019)\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, Max Welling. Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions. (2021)\nNiklas W. A. Gebauer, Michael Gastegger, Kristof T. Schütt. Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules. (2019)\nThis list is by no means comprehensive, check out the paper for more details.\n","date":1621502040,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621502040,"objectID":"11a6a2a6bd085e25ebb7dce820460db5","permalink":"https://ehoogeboom.github.io/post/en_flows/","publishdate":"2021-05-20T11:14:00+02:00","relpermalink":"/post/en_flows/","section":"post","summary":"How to build E(n) Equivariant Normalizing Flows from our recent paper? We will discuss 1) Normalizing Flows 2) Continuous Time Normalizing Flows 3) E(n) GNNs, 4) Argmax Flows. Finally we talk about our 5) E(n) Flows. Most of these topics are tangential: if you don't care, just read the intuition and skip it :)","tags":[],"title":"How to build E(n) Equivariant Normalizing Flows, for points with features?","type":"post"},{"authors":["Victor Garcia Satorras","Emiel Hoogeboom","Fabian B. Fuchs","Ingmar Posner","Max Welling"],"categories":[],"content":"","date":1617314400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617314400,"objectID":"45b44ea09fba05f84ac24705c4064ba5","permalink":"https://ehoogeboom.github.io/publication/en_equivariant_flows/","publishdate":"2021-04-01T00:00:00+02:00","relpermalink":"/publication/en_equivariant_flows/","section":"publication","summary":"This paper introduces a generative model equivariant to Euclidean symmetries: E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the discriminative E(n) graph neural networks and integrate them as a differential equation to obtain an invertible equivariant function: a continuous-time normalizing flow. We demonstrate that E-NFs considerably outperform baselines and existing methods from the literature on particle systems such as DW4 and LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our knowledge, this is the first flow that jointly generates molecule features and positions in 3D.","tags":[],"title":"E(n) Equivariant Normalizing Flows","type":"publication"},{"authors":["Emiel Hoogeboom","Didrik Nielsen","Priyank Jaini","Patrick Forré","Max Welling"],"categories":[],"content":"","date":1612216800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612216800,"objectID":"60c751209d76c5c48a3f15be982a8a2c","permalink":"https://ehoogeboom.github.io/publication/argmax_flows_mult_diffusion/","publishdate":"2021-02-01T00:00:00+02:00","relpermalink":"/publication/argmax_flows_mult_diffusion/","section":"publication","summary":"Generative flows and diffusion models have been predominantly trained on ordinal data, for example natural images. This paper introduces two extensions of flows and diffusion for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our method outperforms existing dequantization approaches on text modelling and modelling on image segmentation maps in log-likelihood.","tags":[],"title":"Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions","type":"publication"},{"authors":["Emiel Hoogeboom","Victor Garcia Satorras","Max Welling"],"categories":[],"content":"","date":1612216800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612216800,"objectID":"f85129c22bdf92b442bde68b4c1cb500","permalink":"https://ehoogeboom.github.io/publication/egnn/","publishdate":"2021-02-01T00:00:00+02:00","relpermalink":"/publication/egnn/","section":"publication","summary":"This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.","tags":[],"title":"E(n) Equivariant Graph Neural Networks","type":"publication"},{"authors":["Thomas Anderson Keller","Jorn W.T. Peters","Priyank Jaini","Emiel Hoogeboom","Patrick Forré","Max Welling"],"categories":[],"content":"","date":1609452000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609452000,"objectID":"e363033b6982653cc7f43fe246705d46","permalink":"https://ehoogeboom.github.io/publication/self_normalizing_flows/","publishdate":"2021-01-01T00:00:00+02:00","relpermalink":"/publication/self_normalizing_flows/","section":"publication","summary":"Efficient gradient computation of the Jacobian determinant term is a core problem of the normalizing flow framework. Thus, most proposed flow models either restrict to a function class with easy evaluation of the Jacobian determinant, or an efficient estimator thereof. However, these restrictions limit the performance of such density models, frequently requiring significant depth to reach desired performance levels. In this work, we propose Self Normalizing Flows, a flexible framework for training normalizing flows by replacing expensive terms in the gradient by learned approximate inverses at each layer. This reduces the computational complexity of each layer's exact update from O($D^3$) to O($D^2$), allowing for the training of flow architectures which were otherwise computationally infeasible, while also providing efficient sampling. We show experimentally that such models are remarkably stable and optimize to similar data likelihood values as their exact gradient counterparts, while surpassing the performance of their functionally constrained counterparts.","tags":[],"title":"Self Normalizing Flows","type":"publication"},{"authors":["Simon Passenheim","Emiel Hoogeboom"],"categories":[],"content":"","date":1609452000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609452000,"objectID":"fd594c8f21c795c24da7d5953462a3be","permalink":"https://ehoogeboom.github.io/publication/var_det_estimation/","publishdate":"2021-01-01T00:00:00+02:00","relpermalink":"/publication/var_det_estimation/","section":"publication","summary":"This paper introduces the Variational Determinant Estimator (VDE), a variational extension of the recently proposed determinant estimator discovered by Sohl-Dickstein (2020). Our estimator significantly reduces the variance even for low sample sizes by combining (importance-weighted) variational inference and a family of normalizing flows which allow density estimation on hyperspheres. In the ideal case of a tight variational bound, the VDE becomes a zero variance estimator, and a single sample is sufficient for an exact (log) determinant estimate.","tags":[],"title":"Variational Determinant Estimation with Spherical Normalizing Flows","type":"publication"},{"authors":["Emiel Hoogeboom","Victor Garcia Satorras","Jakub M. Tomczak","Max Welling"],"categories":[],"content":"","date":1606860000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606860000,"objectID":"4d089adfe87e6f6f6b53706378400f26","permalink":"https://ehoogeboom.github.io/publication/conv_exp/","publishdate":"2020-11-01T00:00:00+02:00","relpermalink":"/publication/conv_exp/","section":"publication","summary":"This paper introduces a new method to build linear flows, by taking the exponential of a linear transformation. This linear transformation does not need to be invertible itself, and the exponential has the following desirable properties: it is guaranteed to be invertible, its inverse is straightforward to compute and the log Jacobian determinant is equal to the trace of the linear transformation. An important insight is that the exponential can be computed implicitly, which allows the use of convolutional layers. Using this insight, we develop new invertible transformations named convolution exponentials and graph convolution exponentials, which retain the equivariance of their underlying transformations. In addition, we generalize Sylvester Flows and propose Convolutional Sylvester Flows which are based on the generalization and the convolution exponential as basis change. Empirically, we show that the convolution exponential outperforms other linear transformations in generative flows on CIFAR10 and the graph convolution exponential improves the performance of graph normalizing flows. In addition, we show that Convolutional Sylvester Flows improve performance over residual flows as a generative flow model measured in log-likelihood.","tags":[],"title":"The Convolution Exponential and Generalized Sylvester Flows","type":"publication"},{"authors":["Didrik Nielsen","Priyank Jaini","Emiel Hoogeboom","Ole Winther","Max Welling"],"categories":[],"content":"","date":1606773600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606773600,"objectID":"e8abc93d7ceb1489423fad187575f0ce","permalink":"https://ehoogeboom.github.io/publication/survae_flows/","publishdate":"2020-11-01T00:00:00+02:00","relpermalink":"/publication/survae_flows/","section":"publication","summary":"Normalizing flows and variational autoencoders are powerful generative models that can represent complicated density functions. However, they both impose constraints on the models: Normalizing flows use bijective transformations to model densities whereas VAEs learn stochastic transformations that are non-invertible and thus typically do not provide tractable estimates of the marginal likelihood. In this paper, we introduce SurVAE Flows: A modular framework of composable transformations that encompasses VAEs and normalizing flows. SurVAE Flows bridge the gap between normalizing flows and VAEs with surjective transformations, wherein the transformations are deterministic in one direction -- thereby allowing exact likelihood computation, and stochastic in the reverse direction -- hence providing a lower bound on the corresponding likelihood. We show that several recently proposed methods, including dequantization and augmented normalizing flows, can be expressed as SurVAE Flows. Finally, we introduce common operations such as the max value, the absolute value, sorting and stochastic permutation as composable layers in SurVAE Flows.","tags":[],"title":"SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows","type":"publication"},{"authors":["Emiel Hoogeboom","Taco S. Cohen","Jakub Tomczak"],"categories":[],"content":"","date":1580508000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580508000,"objectID":"f7f7b30d58cf61a545c6ea868891cb40","permalink":"https://ehoogeboom.github.io/publication/dequantization/","publishdate":"2020-02-01T00:00:00+02:00","relpermalink":"/publication/dequantization/","section":"publication","summary":"Media is generally stored digitally and is therefore discrete. Many successful deep distribution models in deep learning learn a density, i.e., the distribution of a continuous random variable. Naïve optimization on discrete data leads to arbitrarily high likelihoods, and instead, it has become standard practice to add noise to datapoints. In this paper, we present a general framework for dequantization that captures existing methods as a special case. We derive two new dequantization objectives: importance-weighted (iw) dequantization and Rényi dequantization. In addition, we introduce autoregressive dequantization (ARD) for more flexible dequantization distributions. Empirically we find that iw and Rényi dequantization considerably improve performance for uniform dequantization distributions. ARD achieves a negative log-likelihood of 3.06 bits per dimension on CIFAR10, which to the best of our knowledge is state-of-the-art among distribution models that do not require autoregressive inverses for sampling.","tags":[],"title":"Learning Discrete Distributions by Dequantization","type":"publication"},{"authors":["Auke Wiggers","Emiel Hoogeboom"],"categories":[],"content":"","date":1580508000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580508000,"objectID":"50916b5483333ecd1ae43a49c6f42949","permalink":"https://ehoogeboom.github.io/publication/predictive_sampling/","publishdate":"2020-02-01T00:00:00+02:00","relpermalink":"/publication/predictive_sampling/","section":"publication","summary":"Autoregressive models (ARMs) currently hold state-of-the-art performance in likelihood-based modeling of image and audio data. Generally, neural network based ARMs are designed to allow fast inference, but sampling from these models is impractically slow. In this paper, we introduce the predictive sampling algorithm: a procedure that exploits the fast inference property of ARMs in order to speed up sampling, while keeping the model intact. We propose two variations of predictive sampling, namely sampling with ARM fixed-point iteration and learned forecasting modules. Their effectiveness is demonstrated in two settings: i) explicit likelihood modeling on binary MNIST, SVHN and CIFAR10, and ii) discrete latent modeling in an autoencoder trained on SVHN, CIFAR10 and Imagenet32. Empirically, we show considerable improvements over baselines in number of ARM inference calls and sampling speed.","tags":[],"title":"Predictive Sampling with Forecasting Autoregressive Models","type":"publication"},{"authors":["Christina Winkler","Daniel Worrall","Emiel Hoogeboom","Max Welling"],"categories":[],"content":"","date":1577829600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577829600,"objectID":"da416a9bac6495f8bba8423a1b6c4f93","permalink":"https://ehoogeboom.github.io/publication/conditional_flows/","publishdate":"2020-01-01T00:00:00+02:00","relpermalink":"/publication/conditional_flows/","section":"publication","summary":"Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.","tags":[],"title":"Learning Likelihoods with Conditional Normalizing Flows","type":"publication"},{"authors":["Emiel Hoogeboom","Jorn W.T. Peters","Rianne van den Berg","Max Welling"],"categories":[],"content":"","date":1575151200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575151200,"objectID":"a27cdc0ceb526c0857536c57433b6fd0","permalink":"https://ehoogeboom.github.io/publication/integer_flows/","publishdate":"2019-12-01T00:00:00+02:00","relpermalink":"/publication/integer_flows/","section":"publication","summary":"Lossless compression methods shorten the expected representation size of data without loss of information, using a statistical model. Flow-based models are attractive in this setting because they admit exact likelihood optimization, which is equivalent to minimizing the expected number of bits per message. However, conventional flows assume continuous data, which may lead to reconstruction errors when quantized for compression. For that reason, we introduce a flow-based generative model for ordinal discrete data called Integer Discrete Flow (IDF): a bijective integer map that can learn rich transformations on high-dimensional data. As building blocks for IDFs, we introduce a flexible transformation layer called integer discrete coupling. Our experiments show that IDFs are competitive with other flow-based generative models. Furthermore, we demonstrate that IDF based compression achieves state-of-the-art lossless compression rates on CIFAR10, ImageNet32, and ImageNet64. To the best of our knowledge, this is the first lossless compression method that uses invertible neural networks.","tags":[],"title":"Integer Discrete Flows and Lossless Compression","type":"publication"},{"authors":["Emiel Hoogeboom"],"categories":[],"content":" Recently, the Machine Learning community has become more focused on finding invertible transformations. One of the benefits of invertible transformations is that the change of variable formula holds:\n$$p_X(x) = p_Z(z) \\left | \\frac{dz}{dx} \\right|, \\quad z = f(x),$$\nwhich admits the optimization of a complicated likelihood $p_X(\\cdot)$ via a simple, tractable one: $p_Z(\\cdot)$. Glow introduced invertible $1 \\times 1$ convolutions and i-ResNet introduced invertible residual connections.\nOur work aims to invert the convolution operation1 itself. In other words, we want to be able to find the inverse convolution, sometimes referred to as a deconvolution. We introduce three methods: i) emerging convolutions for invertible zero-padded convolutions, ii) invertible periodic convolutions, and iii) stable and flexible $1 \\times 1$ convolutions. For details, check out paper or code.\nStandard convolutions To understand how, let\u0026rsquo;s first study a deep learning convolution. We will assume our kernel $\\mathbf{w}$ and input $\\mathbf{x}$ are both single channel with height 3 and width 3 pixels. Also, we will assume 1 pixel wide zero padding. It turns out, this convolution (depicted left), can be expressed as a matrix multiplication (right):\n A 1-channel convolution as matrix multiplication   Going further, we can even visualize a multi-channel convolution. For clarity, we omit the value descriptions inside the filter. Let\u0026rsquo;s visualize a 2-channel convolution:\n A 2-channel convolution as matrix multiplication   In this figure, the kernel and equivalent matrix are shown without the input. Implicitly, a vector ordering on the input $x$ was chosen to visualize this matrix. It is not very important what this ordering is, as it only determines the order in which the parameter values appear in the matrix.\nNow that we have expressed everything as linear algebra, it seems we have solved the problem and we can find the inverse. However, for images of larger size, this matrix would grow quite quickly: It has dimensions $h w c \\times h w c$, where $h$ is height, $w$ is width, and $c$ is number of channels. Therefore, even though inverting this matrix directly is technically possible, the growth of the matrix makes this practically infeasible.\nAutoregressive Convolutions Convolution kernels can be masked in a very particular way, such that the equivalent matrix is triangular. As a consequence, it is straightforward to compute the Jacobian determinant and the inverse for this operation. In essence, we have traded complexity for a convolution that is easy to invert:\n A 2-channel autoregressive convolution as matrix multiplication   Even though the autoregressive convolution is easily inverted, it suffers from limited flexibility: in this case, it is blind to pixels below.\nEmerging Convolutions Now we are all set to create an invertible convolution. The main idea is to combine multiple autoregressive convolutions, with specifically designed masks. As a result, the emerging receptive field is identical to that of a standard convolution:  Applying two autoregressive convolutions (left) results in a convolution with a receptive field identical to standard convolutions (right)  \nThis allows one to learn an invertible convolution from a factorization of two masked convolutions. Mathematically, this can be viewed as applying a convolution with filter $k_1$ and then with $k_2$, or as a single convolution2 with filter $k = k_2 * {k_1}$:\n$$k_2 \\star (k_1 \\star f) = (k_2 * {k_1}) \\star f.$$\nInvertible Periodic Convolutions Alternative to using factored convolutions, we may also leverage the convolution theorem, which is connected with the Fourier transform. As a consequence, our convolutions will be circular. This can be useful when data is periodic around edges, or if the pixels contain roughly the same values. For the exact details please refer to the paper. In short: the feature maps and kernels are transformed to the frequency domain using the Fourier transform. In the frequency domain, the equivalent matrix of the convolution is a block matrix, and the convolution can be computed as:\n$$\\hat{\\mathbf{z}}_{:,uv} = \\hat{\\mathbf{W}}_{uv} \\, \\hat{\\mathbf{x}}_{:,uv},$$ where the notation $\\hat{\\,}$ denotes a tensor is in the frequency domain, by applying the fourier transform. The inverse is obtained by simply inverting $\\hat{\\mathbf{W}}_{uv}$. $$\\hat{\\mathbf{z}}_{:,uv} = \\hat{\\mathbf{W}}_{uv}^{-1} \\, \\hat{\\mathbf{x}}_{:,uv},$$\nAlthough it may seem like we have not gained anything, the matrices $\\hat{\\mathbf{W}}_{uv}$ are actually independent and small. Instead of a naïve inverse, which would have cost $\\mathcal{O}(h^3 w^3 c^3)$, inverting this method has computational cost of $\\mathcal{O}(hw c^3)$, which can even be parallelized across the dimensions $w$ and $h$.\nStable 1 $\\times$ 1 QR convolutions In addition to the invertible $d \\times d$ convolutions, we also introduce a stable and flexible parametrization of the 1 $\\times$ 1 convolution, that uses the QR decomposition. Although any square matrix has a PLU parametrization (as proposed in Glow), it is difficult to train the permutation matrix P using gradient descent. Instead, we introduce a QR parametrization, that can be trained straightforwardly. The inverse is then simply obtained by applying a 1 $\\times$ 1 convolution using the kernel $R^{-1} Q^\\text{T}$.\nConclusion In summary, we have introduced three generative flows: i) $d \\times d$ emerging convolutions as invertible standard zero-padded convolutions, ii) $d \\times d$ periodic convolutions for periodic data or data with minimal boundary variation, and iii) stable and flexible $1 \\times 1$ convolutions using a QR parametrization. If you want to know more, check out paper or code.\n Left: samples from a flow using periodic convolutions trained on galaxy images. Right: samples from a flow using emerging convolutions trained on CIFAR10. Bottom: Performance in bits per dimension, i.e. negative log$_2$-likelihood (lower is better)   References Hoogeboom, E., Van Den Berg, R. \u0026amp; Welling, M.. (2019). Emerging Convolutions for Generative Normalizing Flows. Proceedings of the 36th International Conference on Machine Learning, in PMLR 97:2771-2780\nKingma, D. P., Dhariwal, P.. Glow: Generative Flow with Invertible 1x1 Convolutions. Advances in Neural Information Processing Systems 31.\nBehrmann, J., Grathwohl, W., Chen, R.T.Q., Duvenaud, D. \u0026amp; Jacobsen, J.. (2019). Invertible Residual Networks. Proceedings of the 36th International Conference on Machine Learning, in PMLR 97:573-582\n In practice, deep learning convolutions are generally a summation of cross-correlations. In text this detail is omitted and everything is referred to as a convolution. ^ Equation holds when signals are real and discrete. ^   ","date":1559121240,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559121240,"objectID":"ac27c6dd111b3a4cf6558bc62c337e75","permalink":"https://ehoogeboom.github.io/post/invertible_convs/","publishdate":"2019-05-29T11:14:00+02:00","relpermalink":"/post/invertible_convs/","section":"post","summary":"We introduce three types of invertible convolutions: i) emerging convolutions for invertible zero-padded convolutions, ii) invertible periodic convolutions, and iii) stable and flexible 1 x 1 convolutions. convolutions","tags":["invertible convolution","deconvolution","normalizing flows","generative models"],"title":"Invertible Convolutions","type":"post"},{"authors":["Emiel Hoogeboom","Rianne van den Berg","Max Welling"],"categories":[],"content":"","date":1548626400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548626400,"objectID":"c88fd35e9391633d6622e7d6e2360bcb","permalink":"https://ehoogeboom.github.io/publication/emerging_convs/","publishdate":"2019-05-29T00:00:00+02:00","relpermalink":"/publication/emerging_convs/","section":"publication","summary":"Generative flows are attractive because they admit exact likelihood optimization and efficient image synthesis. Recently, Kingma \u0026 Dhariwal (2018) demonstrated with Glow that generative flows are capable of generating high quality images. We generalize the 1 x 1 convolutions proposed in Glow to invertible d x d convolutions, which are more flexible since they operate on both channel and spatial axes. We propose two methods to produce invertible convolutions that have receptive fields identical to standard convolutions: Emerging convolutions are obtained by chaining specific autoregressive convolutions, and periodic convolutions are decoupled in the frequency domain. Our experiments show that the flexibility of d x d convolutions significantly improves the performance of generative flow models on galaxy images, CIFAR10 and ImageNet.","tags":[],"title":"Emerging Convolutions for Generative Normalizing Flows","type":"publication"},{"authors":["Emiel Hoogeboom","Jorn W.T. Peters","Taco S. Cohen","Max Welling"],"categories":[],"content":"","date":1520287200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520287200,"objectID":"eb87e5aa04e65ed738046c0013d26440","permalink":"https://ehoogeboom.github.io/publication/hexaconv/","publishdate":"2019-05-29T00:00:00+02:00","relpermalink":"/publication/hexaconv/","section":"publication","summary":"The effectiveness of Convolutional Neural Networks stems in large part from their ability to exploit the translation invariance that is inherent in many learning problems. Recently, it was shown that CNNs can exploit other invariances, such as rotation invariance, by using group convolutions instead of planar convolutions. However, for reasons of performance and ease of implementation, it has been necessary to limit the group convolution to transformations that can be applied to the filters without interpolation. Thus, for images with square pixels, only integer translations, rotations by multiples of 90 degrees, and reflections are admissible. Whereas the square tiling provides a 4-fold rotational symmetry, a hexagonal tiling of the plane has a 6-fold rotational symmetry. In this paper we show how one can efficiently implement planar convolution and group convolution over hexagonal lattices, by re-using existing highly optimized convolution routines. We find that, due to the reduced anisotropy of hexagonal filters, planar HexaConv provides better accuracy than planar convolution with square filters, given a fixed parameter budget. Furthermore, we find that the increased degree of symmetry of the hexagonal grid increases the effectiveness of group convolutions, by allowing for more parameter sharing. We show that our method significantly outperforms conventional CNNs on the AID aerial scene classification dataset, even outperforming ImageNet pre-trained models.","tags":[],"title":"HexaConv","type":"publication"}]